{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data"
      ],
      "metadata": {
        "id": "5GgDLvFJxWtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkdvlWwjrhB9",
        "outputId": "45bf9f9f-3b54-424f-d4ed-7e0942abc80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Behaviors Data:\n",
            "   Impression ID  User ID                    Time  \\\n",
            "0              1   U87243  11/10/2019 11:30:54 AM   \n",
            "1              2  U598644   11/12/2019 1:45:29 PM   \n",
            "2              3  U532401  11/13/2019 11:23:03 AM   \n",
            "3              4  U593596  11/12/2019 12:24:09 PM   \n",
            "4              5  U239687   11/14/2019 8:03:01 PM   \n",
            "\n",
            "                                             History  \\\n",
            "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
            "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
            "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
            "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
            "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
            "\n",
            "                                         Impressions  \n",
            "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
            "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
            "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
            "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
            "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
            "\n",
            "News Data:\n",
            "  News ID   Category               SubCategory  \\\n",
            "0  N88753  lifestyle           lifestyleroyals   \n",
            "1  N45436       news  newsscienceandtechnology   \n",
            "2  N23144     health                weightloss   \n",
            "3  N86255     health                   medical   \n",
            "4  N93187       news                 newsworld   \n",
            "\n",
            "                                               Title  \\\n",
            "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
            "1    Walmart Slashes Prices on Last-Generation iPads   \n",
            "2                      50 Worst Habits For Belly Fat   \n",
            "3  Dispose of unwanted prescription drugs during ...   \n",
            "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
            "\n",
            "                                            Abstract  \\\n",
            "0  Shop the notebooks, jackets, and more that the...   \n",
            "1  Apple's new iPad releases bring big deals on l...   \n",
            "2  These seemingly harmless habits are holding yo...   \n",
            "3                                                NaN   \n",
            "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
            "\n",
            "                                      Title Entities  \\\n",
            "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
            "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
            "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
            "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
            "4                                                 []   \n",
            "\n",
            "                                   Abstract Entities  \n",
            "0                                                 []  \n",
            "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
            "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
            "3                                                 []  \n",
            "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Read behaviors.tsv\n",
        "behaviors_path = '/content/behaviors.tsv'\n",
        "train_df = pd.read_csv(behaviors_path, sep='\\t', header=None, names=[\n",
        "    \"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"\n",
        "])\n",
        "print(\"Behaviors Data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# 2. Read news.tsv\n",
        "news_path = '/content/news.tsv'\n",
        "news_df = pd.read_csv(news_path, sep='\\t', header=None, names=[\n",
        "    \"News ID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\",\n",
        "    \"Title Entities\", \"Abstract Entities\"\n",
        "])\n",
        "news_df = news_df.drop(columns=['URL'])\n",
        "print(\"\\nNews Data:\")\n",
        "print(news_df.head())\n",
        "\n",
        "# 3. Read entity_embedding.vec\n",
        "entity_embedding_path = '/content/entity_embedding.vec'\n",
        "entity_embeddings = {}\n",
        "with open(entity_embedding_path, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split()\n",
        "        entity_id = parts[0]\n",
        "        embedding = np.array(parts[1:], dtype=float)\n",
        "        entity_embeddings[entity_id] = embedding\n",
        "\n",
        "\n",
        "# 4. Read relation_embedding.vec\n",
        "relation_embedding_path = '/content/relation_embedding.vec'\n",
        "relation_embeddings = {}\n",
        "with open(relation_embedding_path, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split()\n",
        "        relation_id = parts[0]\n",
        "        embedding = np.array(parts[1:], dtype=float)\n",
        "        relation_embeddings[relation_id] = embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Time'] = pd.to_datetime(train_df['Time'])  # Ensure the column is in datetime format\n",
        "min_date = train_df['Time'].min()\n",
        "max_date = train_df['Time'].max()\n",
        "\n",
        "print(\"Minimum date:\", min_date)\n",
        "print(\"Maximum date:\", max_date)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnWQWjCB-QCO",
        "outputId": "fd9235cd-dc06-46e9-ce74-2e46fdfd9fd5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum date: 2019-11-09 00:00:00\n",
            "Maximum date: 2019-11-14 23:59:59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where 'History' is NaN to avoid errors during counting\n",
        "print(train_df.shape)\n",
        "train_df = train_df.dropna(subset=['History'])\n",
        "print(train_df.shape)\n",
        "# Count the number of history items for each User ID\n",
        "train_df['History Count'] = train_df['History'].apply(lambda x: len(x.split(' ')))\n",
        "\n",
        "# Display the User ID and corresponding history count\n",
        "history_counts = train_df[['User ID', 'History Count']]\n",
        "print(history_counts.head())\n",
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzAfIOyIcvF9",
        "outputId": "e525a923-e8c4-403a-dd9c-a3e724c8aab5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1968593, 5)\n",
            "(1927969, 5)\n",
            "   User ID  History Count\n",
            "0   U87243             16\n",
            "1  U598644             24\n",
            "2  U532401             16\n",
            "3  U593596             13\n",
            "4  U239687            339\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1927969, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# 3. Prepare data for collaborative filtering\n",
        "interaction_data = defaultdict(lambda: defaultdict(int))  # Using defaultdict to store counts of each news_id for each user\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in train_df.iterrows():\n",
        "    user_id = row['User ID']\n",
        "    impressions = row['Impressions']\n",
        "\n",
        "    # Check if impressions is not NaN\n",
        "    if pd.notna(impressions):\n",
        "        impressions = impressions.split()  # Split the impressions string into a list\n",
        "        for impression in impressions:\n",
        "            news_id, clicked = impression.split('-')  # Separate news ID and click indicator\n",
        "            clicked = int(clicked)  # Convert click indicator to integer\n",
        "            interaction_data[user_id][news_id] += clicked  # Accumulate clicks for the same news_id\n",
        "\n",
        "# Combine the impressions into one list for each user\n",
        "combined_interactions = []\n",
        "\n",
        "# Create the list for each user, including all impressions with their click counts\n",
        "for user_id, news_dict in interaction_data.items():\n",
        "    # Create a list of \"news_id-click_count\" for the user\n",
        "    user_impressions = [f\"{news_id}-{click_count}\" for news_id, click_count in news_dict.items()]\n",
        "    combined_interactions.append((user_id, \" \".join(user_impressions)))\n",
        "\n",
        "# Create a DataFrame from the combined interactions\n",
        "interactions_df = pd.DataFrame(combined_interactions, columns=['User ID', 'Impressions_new'])\n",
        "\n",
        "# Merge the new interactions back with the original train_df\n",
        "train_df = pd.merge(train_df, interactions_df, on='User ID', how='left')\n",
        "\n",
        "# Now, train_df will contain the 'Impressions_new' column with all impressions for each user\n",
        "print(train_df.head())  # Display the first few rows of the updated train_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NihCY9GYeO2H",
        "outputId": "2e93d6ef-31ff-4de4-d210-80cb56e545a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Impression ID  User ID                Time  \\\n",
            "0              1   U87243 2019-11-10 11:30:54   \n",
            "1              2  U598644 2019-11-12 13:45:29   \n",
            "2              3  U532401 2019-11-13 11:23:03   \n",
            "3              4  U593596 2019-11-12 12:24:09   \n",
            "4              5  U239687 2019-11-14 20:03:01   \n",
            "\n",
            "                                             History  \\\n",
            "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
            "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
            "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
            "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
            "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
            "\n",
            "                                         Impressions  History Count  \\\n",
            "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...             16   \n",
            "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...             24   \n",
            "2              N103852-0 N53474-0 N127836-0 N47925-1             16   \n",
            "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...             13   \n",
            "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...            339   \n",
            "\n",
            "                                     Impressions_new  \n",
            "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
            "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
            "2  N103852-0 N53474-1 N127836-0 N47925-1 N93856-0...  \n",
            "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
            "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keep unique users"
      ],
      "metadata": {
        "id": "Xob2OAsR7UJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get the most recent history for each user and only retain that row\n",
        "\n",
        "# Group by 'User ID' and get the row with the latest 'Time' for each user\n",
        "train_df_latest = train_df.loc[train_df.groupby('User ID')['Time'].idxmax()]\n",
        "\n",
        "train_df_latest.shape\n",
        "\n",
        "# Remove rows where 'History' is NaN to avoid errors during counting\n",
        "train_df_latest = train_df_latest.dropna(subset=['History'])\n",
        "\n",
        "# Count the number of history items for each User ID\n",
        "train_df_latest['History Count'] = train_df_latest['History'].apply(lambda x: len(x.split(' ')))\n",
        "\n",
        "# Display the User ID and corresponding history count\n",
        "history_counts = train_df_latest[['User ID', 'History Count']]\n",
        "print(history_counts.head())\n",
        "\n",
        "# Remove duplicate User IDs, keeping the first occurrence\n",
        "train_df = train_df.drop_duplicates(subset='User ID', keep='first').reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_df_latest.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQsrtTOskaR6",
        "outputId": "a6217409-1867-4100-e285-300b7fb4e452"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       User ID  History Count\n",
            "901532      U0              8\n",
            "571026      U1             72\n",
            "613095     U10              3\n",
            "631603    U100             43\n",
            "611813   U1000              9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(668745, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "\n",
        "# Convert to DataFrame\n",
        "train_df_10000 = train_df[:1000]\n",
        "\n",
        "# Step 1: Split each user's history into lists and handle non-string entries\n",
        "train_df_10000['History'] = train_df_10000['History'].apply(lambda x: str(x).split() if isinstance(x, str) else [])\n",
        "\n",
        "# Step 2: Use MultiLabelBinarizer to create the user-item matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "user_item_matrix = pd.DataFrame(mlb.fit_transform(train_df_10000['History']),\n",
        "                                index=train_df_10000['User ID'],\n",
        "                                columns=mlb.classes_)\n",
        "\n",
        "print(user_item_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4u8J9eB7q8S",
        "outputId": "b6af0425-0a45-4899-fb45-32e06e63d19b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-074766e9094c>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df_10000['History'] = train_df_10000['History'].apply(lambda x: str(x).split() if isinstance(x, str) else [])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         N100  N100022  N100026  N100033  N100063  N100068  N100080  N100091  \\\n",
            "User ID                                                                        \n",
            "U87243      0        0        0        0        0        0        0        0   \n",
            "U598644     0        0        0        0        0        0        0        0   \n",
            "U532401     0        0        0        0        0        0        0        0   \n",
            "U593596     0        0        0        0        0        0        0        0   \n",
            "U239687     0        0        0        1        0        0        0        0   \n",
            "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
            "U115243     0        0        0        0        0        0        0        0   \n",
            "U311138     0        0        0        0        0        0        1        0   \n",
            "U383721     0        0        0        0        0        0        0        0   \n",
            "U307781     0        0        0        0        0        0        0        0   \n",
            "U552836     0        0        0        0        0        0        0        0   \n",
            "\n",
            "         N100119  N10012  ...  N99846  N99863  N99908  N99932  N99952  N99958  \\\n",
            "User ID                   ...                                                   \n",
            "U87243         0       0  ...       0       0       0       0       0       0   \n",
            "U598644        0       0  ...       0       0       0       0       0       0   \n",
            "U532401        0       0  ...       0       0       0       0       0       0   \n",
            "U593596        0       0  ...       0       0       0       0       0       0   \n",
            "U239687        1       0  ...       0       0       0       0       0       0   \n",
            "...          ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
            "U115243        0       0  ...       0       0       0       0       0       0   \n",
            "U311138        0       0  ...       0       0       0       0       0       0   \n",
            "U383721        0       0  ...       0       0       0       0       0       0   \n",
            "U307781        0       0  ...       0       0       0       0       0       0   \n",
            "U552836        0       0  ...       0       0       0       0       0       0   \n",
            "\n",
            "         N99962  N99970  N99971  N99978  \n",
            "User ID                                  \n",
            "U87243        0       0       0       0  \n",
            "U598644       0       0       0       0  \n",
            "U532401       0       0       0       0  \n",
            "U593596       0       0       0       0  \n",
            "U239687       0       0       0       0  \n",
            "...         ...     ...     ...     ...  \n",
            "U115243       0       0       0       0  \n",
            "U311138       0       0       0       0  \n",
            "U383721       1       0       0       0  \n",
            "U307781       0       0       0       0  \n",
            "U552836       0       0       0       0  \n",
            "\n",
            "[1000 rows x 9227 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Armed Bandit"
      ],
      "metadata": {
        "id": "b7JD-PlD8MiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "\n",
        "class PersonalizedNewsRecommender:\n",
        "    def __init__(self, epsilon, n_recommendations, train_df, all_articles,\n",
        "                 user_col_name='User ID', history_col_name='History', random_seed=2024):\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.train_df = train_df\n",
        "        self.user_col_name = user_col_name\n",
        "        self.history_col_name = history_col_name\n",
        "        self.n_recommendations = n_recommendations\n",
        "        self.all_articles = set(all_articles)  # Complete set of available articles\n",
        "\n",
        "        # Convert history strings to lists if they're strings\n",
        "        if self.train_df[self.history_col_name].dtype == 'object':\n",
        "            self.train_df[self.history_col_name] = self.train_df[self.history_col_name].apply(\n",
        "                lambda x: literal_eval(x) if isinstance(x, str) else x\n",
        "            )\n",
        "\n",
        "        self.users = self.train_df[self.user_col_name].unique()\n",
        "\n",
        "        # Calculate global item popularity\n",
        "        self.item_popularity = self._calculate_item_popularity()\n",
        "\n",
        "    def _calculate_item_popularity(self):\n",
        "        \"\"\"Calculate how many times each item appears across all histories\"\"\"\n",
        "        all_items = [item for history in self.train_df[self.history_col_name] for item in history]\n",
        "        return dict(Counter(all_items))\n",
        "\n",
        "    def get_recommendations_for_user(self, user_id):\n",
        "        # Get user's history\n",
        "        user_history = set(self.train_df[\n",
        "            self.train_df[self.user_col_name] == user_id\n",
        "        ][self.history_col_name].iloc[0])\n",
        "\n",
        "        # Available items (all articles except ones user has already seen)\n",
        "        available_items = list(self.all_articles - user_history)\n",
        "\n",
        "        if not available_items:\n",
        "            return []  # Return empty if no new items available\n",
        "\n",
        "        recommendations = []\n",
        "        for _ in range(min(self.n_recommendations, len(available_items))):\n",
        "            # Exploration or exploitation choice\n",
        "            if np.random.rand() < self.epsilon:  # Explore\n",
        "                chosen_item = np.random.choice(available_items)\n",
        "            else:  # Exploit\n",
        "                # Choose based on global popularity (default to 0 for never-clicked articles)\n",
        "                item_scores = {\n",
        "                    item: self.item_popularity.get(item, 0)\n",
        "                    for item in available_items\n",
        "                }\n",
        "                chosen_item = max(item_scores.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "            # Add to recommendations and remove from available items\n",
        "            recommendations.append(chosen_item)\n",
        "            available_items.remove(chosen_item)\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def generate_all_recommendations(self):\n",
        "        \"\"\"Generate recommendations for all users\"\"\"\n",
        "        all_recommendations = {}\n",
        "\n",
        "        for i, user_id in enumerate(self.users):\n",
        "            recommendations = self.get_recommendations_for_user(user_id)\n",
        "            all_recommendations[user_id] = recommendations\n",
        "\n",
        "            # Print progress every 1000 users\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f\"Generated recommendations for {i + 1} users\")\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        recommendation_records = []\n",
        "        for user_id, recs in all_recommendations.items():\n",
        "            for rank, item_id in enumerate(recs, 1):\n",
        "                recommendation_records.append({\n",
        "                    'User ID': user_id,\n",
        "                    'News ID': item_id,\n",
        "                    'Rank': rank\n",
        "                })\n",
        "\n",
        "        recommendations_df = pd.DataFrame(recommendation_records)\n",
        "        return recommendations_df\n"
      ],
      "metadata": {
        "id": "WBXVZOse8L7u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the recommender\n",
        "recommender = PersonalizedNewsRecommender(\n",
        "    epsilon=0.3,  # 10% exploration rate (you can adjust this)\n",
        "    n_recommendations=50,  # How many recommendations you want per user\n",
        "    train_df=train_df_10000,  # Keyword argument for the training DataFrame\n",
        "    all_articles=user_item_matrix.columns  # Keyword argument for all available articles\n",
        ")\n",
        "\n",
        "# Generate recommendations for all users\n",
        "recommendations_df = recommender.generate_all_recommendations()\n",
        "\n",
        "# Print the recommendations\n",
        "print(\"\\nRecommendations for all users:\")\n",
        "print(recommendations_df)\n",
        "\n",
        "# If you want recommendations for a specific user:\n",
        "specific_user = 'U87243'\n",
        "user_recs = recommender.get_recommendations_for_user(specific_user)\n",
        "print(f\"\\nRecommendations for user {specific_user}:\")\n",
        "print(user_recs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oeDyLEV8MHR",
        "outputId": "f86f24ce-3136-48c2-c94c-a4584a1d3e90"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-37b3f9bc9661>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.train_df[self.history_col_name] = self.train_df[self.history_col_name].apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated recommendations for 1000 users\n",
            "\n",
            "Recommendations for all users:\n",
            "       User ID  News ID  Rank\n",
            "0       U87243   N91597     1\n",
            "1       U87243    N9375     2\n",
            "2       U87243   N25725     3\n",
            "3       U87243   N80126     4\n",
            "4       U87243  N104737     5\n",
            "...        ...      ...   ...\n",
            "49995  U552836   N13604    46\n",
            "49996  U552836   N50645    47\n",
            "49997  U552836  N114787    48\n",
            "49998  U552836    N3435    49\n",
            "49999  U552836    N7553    50\n",
            "\n",
            "[50000 rows x 3 columns]\n",
            "\n",
            "Recommendations for user U87243:\n",
            "['N91597', 'N9375', 'N80126', 'N74685', 'N104737', 'N88875', 'N45124', 'N128965', 'N17456', 'N71977', 'N128643', 'N72345', 'N54360', 'N1713', 'N82348', 'N79909', 'N96479', 'N72571', 'N79285', 'N86208', 'N18904', 'N22651', 'N73137', 'N90947', 'N93049', 'N72976', 'N18399', 'N58200', 'N454', 'N9740', 'N85544', 'N53360', 'N71643', 'N120031', 'N4289', 'N94737', 'N85484', 'N97355', 'N98095', 'N115167', 'N114571', 'N65119', 'N31043', 'N85056', 'N114848', 'N95066', 'N53933', 'N48876', 'N67369', 'N51166']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_correct_recommendations(recommended_news, impressions_new):\n",
        "\n",
        "    # Split the impressions string into individual \"news_id-click_count\" pairs\n",
        "    if isinstance(impressions_new, np.ndarray):\n",
        "        impressions_new = impressions_new[0]  # Get the string from the array\n",
        "\n",
        "    # Split the impressions string into individual \"news_id-click_count\" pairs\n",
        "    impressions = impressions_new.split()\n",
        "\n",
        "    # Create a dictionary of impressions where the key is the news_id and value is the click_count\n",
        "    impressions_dict = {impression.split('-')[0]: int(impression.split('-')[1]) for impression in impressions}\n",
        "\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Iterate over each recommended news ID and check if it appears in the impressions\n",
        "    for news_id in recommended_news:\n",
        "        if news_id in impressions_dict:\n",
        "            total_count += 1\n",
        "            if impressions_dict[news_id] == 1:  # Check if the news item was clicked\n",
        "                correct_count += 1\n",
        "\n",
        "    return correct_count, total_count"
      ],
      "metadata": {
        "id": "hPG4v7W1BqZB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mab_correct_count = 0\n",
        "mab_total_count = 0\n",
        "\n",
        "\n",
        "\n",
        "for uid in train_df_10000['User ID'].values:\n",
        "    # Apply find_unseen_news_for_user_item_based (item-based method)\n",
        "    user_recs = recommender.get_recommendations_for_user(uid)\n",
        "\n",
        "    impressions_new = train_df_10000[train_df_10000['User ID'] == uid][\"Impressions_new\"].values\n",
        "    correct_count_item_based, total_count_item_based = count_correct_recommendations(user_recs, impressions_new)\n",
        "    mab_correct_count += correct_count_item_based\n",
        "    mab_total_count += total_count_item_based\n",
        "\n",
        "print(f\"MAB (Method 4):\")\n",
        "print(f\"  Correct Count: {mab_correct_count}\")\n",
        "print(f\"  Total Count: {mab_total_count}\")\n",
        "print(f\"  Accuracy: {mab_correct_count / mab_total_count * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "0NuJjoaV8MJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc8c434-f332-46df-c58e-7cf104b9cc6e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAB (Method 4):\n",
            "  Correct Count: 23\n",
            "  Total Count: 568\n",
            "  Accuracy: 4.05%\n"
          ]
        }
      ]
    }
  ]
}